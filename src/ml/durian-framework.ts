// Durian Model Training and Management Framework
import { ModelMeta } from "../lib/ml-pipeline";
import fs from "fs";
import path from "path";
import { tfliteModelManager } from "./tflite-framework";

export interface DurianModelConfig {
  id: string;
  version: string;
  input_size: number;
  batch_size: number;
  epochs: number;
  learning_rate: number;
  weight_decay: number;
  architecture: "mobilenet_v2" | "efficientnet_b0";
  quantization: {
    int8: boolean;
    float16: boolean;
    representative_samples: number;
  };
  fine_tuning: {
    frozen_epochs: number;
    fine_tune_epochs: number;
    unfreeze_layers: number;
  };
  augmentation: {
    horizontal_flip: boolean;
    rotation: number;
    brightness: number;
    contrast: number;
  };
}

export interface DurianDatasetConfig {
  data_root: string;
  crop: "durian";
  classes: string[];
  train_split: number;
  val_split: number;
  test_split: number;
  min_samples_per_class: number;
  max_samples_per_class?: number;
}

export class DurianModelManager {
  private modelsDir: string;
  private artifactsDir: string;

  constructor(modelsDir = "models", artifactsDir = "artifacts") {
    this.modelsDir = modelsDir;
    this.artifactsDir = artifactsDir;
    this.ensureDirectories();
  }

  private ensureDirectories() {
    [this.modelsDir, this.artifactsDir].forEach(dir => {
      if (!fs.existsSync(dir)) {
        fs.mkdirSync(dir, { recursive: true });
      }
    });
  }

  // Create default Durian model configuration
  createDefaultConfig(): DurianModelConfig {
    return {
      id: "durian_v1_tflite",
      version: "1.0.0",
      input_size: 224,
      batch_size: 32,
      epochs: 12,
      learning_rate: 3e-4,
      weight_decay: 1e-4,
      architecture: "mobilenet_v2",
      quantization: {
        int8: true,
        float16: true,
        representative_samples: 200
      },
      fine_tuning: {
        frozen_epochs: 12,
        fine_tune_epochs: 6,
        unfreeze_layers: 40
      },
      augmentation: {
        horizontal_flip: true,
        rotation: 0.05,
        brightness: 0.1,
        contrast: 0.1
      }
    };
  }

  // Create default Durian dataset configuration
  createDefaultDatasetConfig(): DurianDatasetConfig {
    return {
      data_root: "./data/durian_dataset",
      crop: "durian",
      classes: [
        "anthracnose",
        "phytophthora_foot_rot",
        "leaf_spot",
        "healthy"
      ],
      train_split: 0.7,
      val_split: 0.15,
      test_split: 0.15,
      min_samples_per_class: 100,
      max_samples_per_class: 1000
    };
  }

  // Generate TFLite Colab notebook for Durian training
  generateTFLiteNotebook(config: DurianModelConfig, datasetConfig: DurianDatasetConfig): string {
    const tfliteConfig = tfliteModelManager.createDefaultConfig();
    tfliteConfig.id = config.id;
    tfliteConfig.version = config.version;
    tfliteConfig.input_size = config.input_size;
    tfliteConfig.batch_size = config.batch_size;
    tfliteConfig.epochs = config.epochs;
    tfliteConfig.learning_rate = config.learning_rate;
    tfliteConfig.architecture = config.architecture;

    const tfliteDatasetConfig = tfliteModelManager.createDefaultDatasetConfig();
    tfliteDatasetConfig.crop = datasetConfig.crop;
    tfliteDatasetConfig.classes = datasetConfig.classes;

    return tfliteModelManager.generateTrainingNotebook(tfliteConfig, tfliteDatasetConfig);
  }

  // Generate both PyTorch and TFLite notebooks for Durian
  generateTrainingNotebooks(config: DurianModelConfig, datasetConfig: DurianDatasetConfig): {
    pytorch: string;
    tflite: string;
  } {
    return {
      pytorch: this.generatePyTorchNotebook(config, datasetConfig),
      tflite: this.generateTFLiteNotebook(config, datasetConfig)
    };
  }

  // Generate PyTorch Colab notebook for Durian training
  generatePyTorchNotebook(config: DurianModelConfig, datasetConfig: DurianDatasetConfig): string {
    const notebook = `# Durian Disease Classification Training Notebook
# Generated by RaiAI Framework

#@title 0) Setup: PyTorch, ONNX, utils
!pip -q install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
!pip -q install onnx onnxruntime onnxruntime-tools timm==1.0.8 rich matplotlib scikit-learn

import os, json, time, math, random, shutil, pathlib
from dataclasses import dataclass
from rich.console import Console
console = Console()

#@title 1) Mount Google Drive & paths
from google.colab import drive
drive.mount('/content/drive')

DATA_ROOT = "/content/drive/MyDrive/durian_dataset"  # <-- Durian dataset
ARTIFACTS_DIR = "/content/durian_artifacts"
os.makedirs(ARTIFACTS_DIR, exist_ok=True)

assert os.path.isdir(os.path.join(DATA_ROOT, "train")), "Dataset not found. See folder layout in the instructions."
console.log(f"Data root = {DATA_ROOT}")
console.log(f"Artifacts = {ARTIFACTS_DIR}")

#@title 2) Imports & hyperparams
import torch, torchvision
from torch import nn
from torch.utils.data import DataLoader
from torchvision import transforms, datasets

# Hyperparameters
IMG_SIZE = ${config.input_size}
BATCH = ${config.batch_size}
EPOCHS = ${config.epochs}
LR = ${config.learning_rate}
WD = ${config.weight_decay}
NUM_WORKERS = 2
SEED = 42

random.seed(SEED); torch.manual_seed(SEED)
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
console.log(f"Device = {DEVICE}")

#@title 3) Data transforms & loaders (train/val/test)
train_tf = transforms.Compose([
    transforms.Resize((IMG_SIZE, IMG_SIZE)),
    transforms.RandomHorizontalFlip(),
    transforms.RandomApply([transforms.ColorJitter(0.2,0.2,0.2,0.1)], p=0.5),
    transforms.RandomRotation(10),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]),
])

eval_tf = transforms.Compose([
    transforms.Resize((IMG_SIZE, IMG_SIZE)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]),
])

train_ds = datasets.ImageFolder(os.path.join(DATA_ROOT,"train"), transform=train_tf)
val_ds = datasets.ImageFolder(os.path.join(DATA_ROOT,"val"), transform=eval_tf)
test_ds = datasets.ImageFolder(os.path.join(DATA_ROOT,"test"), transform=eval_tf)

classes = train_ds.classes
num_classes = len(classes)
console.log(f"Classes: {classes}")

train_loader = DataLoader(train_ds, batch_size=BATCH, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True)
val_loader = DataLoader(val_ds, batch_size=BATCH, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)
test_loader = DataLoader(test_ds, batch_size=BATCH, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)

#@title 4) Model: EfficientNet-B0 (timm) with new head
import timm

model = timm.create_model("efficientnet_b0", pretrained=True, num_classes=num_classes)
model = model.to(DEVICE)

criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WD)
scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)

def run_epoch(model, loader, train=True):
    model.train(train)
    total, correct, loss_sum = 0, 0, 0.0
    for x,y in loader:
        x,y = x.to(DEVICE), y.to(DEVICE)
        if train:
            optimizer.zero_grad()
        with torch.set_grad_enabled(train):
            out = model(x)
            loss = criterion(out,y)
            if train:
                loss.backward(); optimizer.step()
        pred = out.argmax(1)
        total += y.size(0)
        correct += (pred==y).sum().item()
        loss_sum += loss.item()*y.size(0)
    return loss_sum/total, correct/total

#@title 5) Train
best_val_acc, best_path = 0.0, os.path.join(ARTIFACTS_DIR, "durian_best.pt")
for epoch in range(1, EPOCHS+1):
    tr_loss, tr_acc = run_epoch(model, train_loader, train=True)
    va_loss, va_acc = run_epoch(model, val_loader, train=False)
    scheduler.step()
    console.log(f"[{epoch:02d}/{EPOCHS}] train_loss={tr_loss:.4f} acc={tr_acc:.3f} | val_loss={va_loss:.4f} acc={va_acc:.3f}")
    if va_acc > best_val_acc:
        best_val_acc = va_acc
        torch.save({"model": model.state_dict(), "classes": classes}, best_path)
        console.log(f"Saved best checkpoint: {best_path} (val_acc={va_acc:.3f})")

#@title 6) Evaluate on test set (confusion matrix & report)
from sklearn.metrics import classification_report, confusion_matrix
import numpy as np

ckpt = torch.load(best_path, map_location=DEVICE)
model.load_state_dict(ckpt["model"]); model.eval()

y_true, y_pred = [], []
with torch.no_grad():
    for x,y in test_loader:
        x = x.to(DEVICE)
        out = model(x)
        pred = out.argmax(1).cpu().numpy().tolist()
        y_pred += pred
        y_true += y.numpy().tolist()

print(classification_report(y_true, y_pred, target_names=classes, digits=3))
print("Confusion matrix:\\n", confusion_matrix(y_true, y_pred))

#@title 7) Export to ONNX (NCHW float32) + quick validation
model.eval().to("cpu")
dummy = torch.randn(1,3,IMG_SIZE,IMG_SIZE, device="cpu")
onnx_path = os.path.join(ARTIFACTS_DIR, "durian_v1.onnx")
torch.onnx.export(
    model, dummy, onnx_path,
    input_names=["input"], output_names=["logits"],
    dynamic_axes={"input": {0: "batch"}, "logits": {0: "batch"}},
    opset_version=13
)
console.log(f"Exported ONNX: {onnx_path}")

# Validate with onnxruntime
import onnxruntime as ort, numpy as np
sess = ort.InferenceSession(onnx_path, providers=["CPUExecutionProvider"])
out = sess.run(None, {"input": np.random.rand(1,3,IMG_SIZE,IMG_SIZE).astype("float32")})
console.log(f"ONNX sanity output shape: {out[0].shape}")

#@title 8) Save labels + meta.json and zip artifacts
labels_path = os.path.join(ARTIFACTS_DIR, "labels.json")
meta = {
    "id": "durian_v1",
    "crop": "durian",
    "task": "disease-classification",
    "version": "1.0.0",
    "input": {"width": IMG_SIZE, "height": IMG_SIZE, "channels": 3},
    "labels": classes,
    "threshold_default": 0.75,
    "runtime": "onnx"
}

with open(labels_path, "w") as f:
    json.dump(classes, f, ensure_ascii=False, indent=2)
with open(os.path.join(ARTIFACTS_DIR,"meta.json"), "w") as f:
    json.dump(meta, f, ensure_ascii=False, indent=2)

!cd /content && zip -r durian_artifacts.zip durian_artifacts >/dev/null
console.log("Artifacts:")
for p in os.listdir(ARTIFACTS_DIR):
    print(" -", p)
print("Zip ready at /content/durian_artifacts.zip")
`;

    return notebook;
  }

  // Save configuration to file
  saveConfig(config: DurianModelConfig, filename?: string): string {
    const configPath = path.join(this.artifactsDir, filename || `${config.id}_config.json`);
    fs.writeFileSync(configPath, JSON.stringify(config, null, 2));
    return configPath;
  }

  // Load configuration from file
  loadConfig(configPath: string): DurianModelConfig {
    const configData = fs.readFileSync(configPath, 'utf8');
    return JSON.parse(configData);
  }

  // Generate model metadata for registry
  generateModelMeta(config: DurianModelConfig, labels: string[]): ModelMeta {
    return {
      id: config.id,
      crop: "durian",
      task: "disease-classification",
      version: config.version,
      input: { 
        width: config.input_size, 
        height: config.input_size, 
        channels: 3 
      },
      labels: labels,
      threshold_default: 0.75,
      runtime: "tflite",
      path: path.join(this.modelsDir, `${config.id}_int8.tflite`)
    };
  }

  // Generate model report
  generateModelReport(config: DurianModelConfig, metrics: any[]): string {
    return `
# Durian Model Report: ${config.id}

## Configuration
- Architecture: ${config.architecture}
- Input Size: ${config.input_size}x${config.input_size}
- Quantization: INT8 + Float16
- Frozen Epochs: ${config.fine_tuning.frozen_epochs}
- Fine-tune Epochs: ${config.fine_tuning.fine_tune_epochs}

## Model Artifacts
- INT8 TFLite: ${config.id}_int8.tflite (optimized for mobile)
- Float16 TFLite: ${config.id}_fp16.tflite (fallback)
- SavedModel: durian_savedmodel/ (for re-export)
- Labels: labels_durian.json
- Metadata: meta_durian.json

## Mobile Deployment
- **INT8 Model**: Best for low-end Android devices
- **Float16 Model**: Fallback for devices without INT8 support
- **Preprocessing**: MobileNetV2 normalization
- **Input**: 224x224x3 RGB images
- **Output**: Softmax probabilities

## Durian Disease Classes
- **anthracnose**: Fungal disease affecting fruit and leaves
- **phytophthora_foot_rot**: Root and stem rot disease
- **leaf_spot**: Various leaf spot diseases
- **healthy**: Healthy durian plant

## Next Steps
1. Deploy INT8 model to mobile app
2. Use Float16 as fallback
3. Test on target devices
4. Monitor performance in production
`;
  }
}

// Export singleton instance
export const durianModelManager = new DurianModelManager();
