// Rice Model Training and Management Framework
import { ModelMeta } from "../lib/ml-pipeline";
import fs from "fs";
import path from "path";
import { tfliteModelManager } from "./tflite-framework";

export interface RiceModelConfig {
  id: string;
  version: string;
  input_size: number;
  batch_size: number;
  epochs: number;
  learning_rate: number;
  weight_decay: number;
  augmentation: boolean;
  pretrained: boolean;
  architecture: "efficientnet_b0" | "efficientnet_b1" | "resnet50" | "mobilenet_v2";
}

export interface RiceDatasetConfig {
  data_root: string;
  train_split: number;
  val_split: number;
  test_split: number;
  classes: string[];
  min_samples_per_class: number;
  max_samples_per_class?: number;
}

export interface RiceTrainingMetrics {
  epoch: number;
  train_loss: number;
  train_acc: number;
  val_loss: number;
  val_acc: number;
  learning_rate: number;
  timestamp: string;
}

export interface RiceModelArtifacts {
  model_path: string;
  labels_path: string;
  meta_path: string;
  config_path: string;
  metrics_path: string;
  onnx_path: string;
  quantized_path?: string;
}

export class RiceModelManager {
  private modelsDir: string;
  private artifactsDir: string;

  constructor(modelsDir = "models", artifactsDir = "artifacts") {
    this.modelsDir = modelsDir;
    this.artifactsDir = artifactsDir;
    this.ensureDirectories();
  }

  private ensureDirectories() {
    [this.modelsDir, this.artifactsDir].forEach(dir => {
      if (!fs.existsSync(dir)) {
        fs.mkdirSync(dir, { recursive: true });
      }
    });
  }

  // Create default rice model configuration
  createDefaultConfig(): RiceModelConfig {
    return {
      id: "rice_v1",
      version: "1.0.0",
      input_size: 224,
      batch_size: 32,
      epochs: 12,
      learning_rate: 3e-4,
      weight_decay: 1e-4,
      augmentation: true,
      pretrained: true,
      architecture: "efficientnet_b0"
    };
  }

  // Create default dataset configuration
  createDefaultDatasetConfig(): RiceDatasetConfig {
    return {
      data_root: "./data/rice_dataset",
      train_split: 0.7,
      val_split: 0.15,
      test_split: 0.15,
      classes: [
        "rice_brown_spot",
        "rice_blast", 
        "bacterial_leaf_blight",
        "healthy"
      ],
      min_samples_per_class: 100,
      max_samples_per_class: 1000
    };
  }

  // Generate PyTorch Colab notebook for training
  generatePyTorchNotebook(config: RiceModelConfig, datasetConfig: RiceDatasetConfig): string {
    const notebook = `# Rice Disease Classification Training Notebook
# Generated by RaiAI Framework

#@title 0) Setup: PyTorch, ONNX, utils
!pip -q install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
!pip -q install onnx onnxruntime onnxruntime-tools timm==1.0.8 rich matplotlib scikit-learn

import os, json, time, math, random, shutil, pathlib
from dataclasses import dataclass
from rich.console import Console
console = Console()

#@title 1) Mount Google Drive & paths
from google.colab import drive
drive.mount('/content/drive')

DATA_ROOT = "/content/drive/MyDrive/rice_dataset"
ARTIFACTS_DIR = "/content/rice_artifacts"
os.makedirs(ARTIFACTS_DIR, exist_ok=True)

#@title 2) Configuration
IMG_SIZE = ${config.input_size}
BATCH = ${config.batch_size}
EPOCHS = ${config.epochs}
LR = ${config.learning_rate}
WD = ${config.weight_decay}
ARCHITECTURE = "${config.architecture}"
NUM_WORKERS = 2
SEED = 42

random.seed(SEED); torch.manual_seed(SEED)
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
console.log(f"Device = {DEVICE}")

#@title 3) Data transforms & loaders
import torch, torchvision
from torch import nn
from torch.utils.data import DataLoader
from torchvision import transforms, datasets

train_tf = transforms.Compose([
    transforms.Resize((IMG_SIZE, IMG_SIZE)),
    transforms.RandomHorizontalFlip(),
    transforms.RandomApply([transforms.ColorJitter(0.2,0.2,0.2,0.1)], p=0.5),
    transforms.RandomRotation(10),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]),
])

eval_tf = transforms.Compose([
    transforms.Resize((IMG_SIZE, IMG_SIZE)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]),
])

train_ds = datasets.ImageFolder(os.path.join(DATA_ROOT,"train"), transform=train_tf)
val_ds   = datasets.ImageFolder(os.path.join(DATA_ROOT,"val"),   transform=eval_tf)
test_ds  = datasets.ImageFolder(os.path.join(DATA_ROOT,"test"),  transform=eval_tf)

classes = train_ds.classes
console.log(f"Classes: {classes}")

train_loader = DataLoader(train_ds, batch_size=BATCH, shuffle=True,  num_workers=NUM_WORKERS, pin_memory=True)
val_loader   = DataLoader(val_ds,   batch_size=BATCH, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)
test_loader  = DataLoader(test_ds,  batch_size=BATCH, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)

#@title 4) Model: ${config.architecture} with new head
import timm

num_classes = len(classes)
model = timm.create_model(ARCHITECTURE, pretrained=True, num_classes=num_classes)
model = model.to(DEVICE)

criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WD)
scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)

def run_epoch(model, loader, train=True):
    model.train(train)
    total, correct, loss_sum = 0, 0, 0.0
    for x,y in loader:
        x,y = x.to(DEVICE), y.to(DEVICE)
        if train:
            optimizer.zero_grad()
        with torch.set_grad_enabled(train):
            out = model(x)
            loss = criterion(out,y)
            if train:
                loss.backward(); optimizer.step()
        pred = out.argmax(1)
        total += y.size(0)
        correct += (pred==y).sum().item()
        loss_sum += loss.item()*y.size(0)
    return loss_sum/total, correct/total

#@title 5) Training Loop
best_val_acc, best_path = 0.0, os.path.join(ARTIFACTS_DIR, "rice_best.pt")
training_metrics = []

for epoch in range(1, EPOCHS+1):
    tr_loss, tr_acc = run_epoch(model, train_loader, train=True)
    va_loss, va_acc = run_epoch(model, val_loader,   train=False)
    scheduler.step()
    
    metrics = {
        "epoch": epoch,
        "train_loss": tr_loss,
        "train_acc": tr_acc,
        "val_loss": va_loss,
        "val_acc": va_acc,
        "learning_rate": scheduler.get_last_lr()[0],
        "timestamp": time.strftime("%Y-%m-%d %H:%M:%S")
    }
    training_metrics.append(metrics)
    
    console.log(f"[{epoch:02d}/{EPOCHS}] train_loss={tr_loss:.4f} acc={tr_acc:.3f} | val_loss={va_loss:.4f} acc={va_acc:.3f}")
    
    if va_acc > best_val_acc:
        best_val_acc = va_acc
        torch.save({"model": model.state_dict(), "classes": classes, "config": ${JSON.stringify(config)}}, best_path)
        console.log(f"Saved best checkpoint: {best_path} (val_acc={va_acc:.3f})")

#@title 6) Evaluation
from sklearn.metrics import classification_report, confusion_matrix
import numpy as np

ckpt = torch.load(best_path, map_location=DEVICE)
model.load_state_dict(ckpt["model"]); model.eval()

y_true, y_pred = [], []
with torch.no_grad():
    for x,y in test_loader:
        x = x.to(DEVICE)
        out = model(x)
        pred = out.argmax(1).cpu().numpy().tolist()
        y_pred += pred
        y_true += y.numpy().tolist()

print(classification_report(y_true, y_pred, target_names=classes, digits=3))
print("Confusion matrix:\\n", confusion_matrix(y_true, y_pred))

#@title 7) Export to ONNX
model.eval().to("cpu")
dummy = torch.randn(1,3,IMG_SIZE,IMG_SIZE, device="cpu")
onnx_path = os.path.join(ARTIFACTS_DIR, "${config.id}.onnx")
torch.onnx.export(
    model, dummy, onnx_path,
    input_names=["input"], output_names=["logits"],
    dynamic_axes={"input": {0: "batch"}, "logits": {0: "batch"}},
    opset_version=13
)
console.log(f"Exported ONNX: {onnx_path}")

#@title 8) Save artifacts
labels_path = os.path.join(ARTIFACTS_DIR, "labels.json")
meta = {
    "id": "${config.id}",
    "crop": "rice",
    "task": "disease-classification",
    "version": "${config.version}",
    "input": {"width": IMG_SIZE, "height": IMG_SIZE, "channels": 3},
    "labels": classes,
    "threshold_default": 0.75,
    "runtime": "onnx",
    "architecture": "${config.architecture}",
    "training_metrics": training_metrics
}

with open(labels_path, "w") as f: json.dump(classes, f, ensure_ascii=False, indent=2)
with open(os.path.join(ARTIFACTS_DIR,"meta.json"), "w") as f: json.dump(meta, f, ensure_ascii=False, indent=2)
with open(os.path.join(ARTIFACTS_DIR,"config.json"), "w") as f: json.dump(${JSON.stringify(config)}, f, ensure_ascii=False, indent=2)
with open(os.path.join(ARTIFACTS_DIR,"metrics.json"), "w") as f: json.dump(training_metrics, f, ensure_ascii=False, indent=2)

!cd /content && zip -r rice_artifacts.zip rice_artifacts >/dev/null
console.log("Artifacts ready at /content/rice_artifacts.zip")
`;

    return notebook;
  }

  // Generate TFLite Colab notebook for training
  generateTFLiteNotebook(config: RiceModelConfig, datasetConfig: RiceDatasetConfig): string {
    const tfliteConfig = tfliteModelManager.createDefaultConfig();
    tfliteConfig.id = config.id + "_tflite";
    tfliteConfig.version = config.version;
    tfliteConfig.input_size = config.input_size;
    tfliteConfig.batch_size = config.batch_size;
    tfliteConfig.epochs = config.epochs;
    tfliteConfig.learning_rate = config.learning_rate;
    tfliteConfig.architecture = config.architecture === "efficientnet_b0" ? "mobilenet_v2" : "mobilenet_v2";

    const tfliteDatasetConfig = tfliteModelManager.createDefaultDatasetConfig();
    tfliteDatasetConfig.crop = datasetConfig.crop;
    tfliteDatasetConfig.classes = datasetConfig.classes;

    return tfliteModelManager.generateTrainingNotebook(tfliteConfig, tfliteDatasetConfig);
  }

  // Generate both PyTorch and TFLite notebooks
  generateTrainingNotebooks(config: RiceModelConfig, datasetConfig: RiceDatasetConfig): {
    pytorch: string;
    tflite: string;
  } {
    return {
      pytorch: this.generatePyTorchNotebook(config, datasetConfig),
      tflite: this.generateTFLiteNotebook(config, datasetConfig)
    };
  }

  // Save configuration to file
  saveConfig(config: RiceModelConfig, filename?: string): string {
    const configPath = path.join(this.artifactsDir, filename || `${config.id}_config.json`);
    fs.writeFileSync(configPath, JSON.stringify(config, null, 2));
    return configPath;
  }

  // Load configuration from file
  loadConfig(configPath: string): RiceModelConfig {
    const configData = fs.readFileSync(configPath, 'utf8');
    return JSON.parse(configData);
  }

  // Generate model metadata for registry
  generateModelMeta(config: RiceModelConfig, labels: string[]): ModelMeta {
    return {
      id: config.id,
      crop: "rice",
      task: "disease-classification",
      version: config.version,
      input: { 
        width: config.input_size, 
        height: config.input_size, 
        channels: 3 
      },
      labels: labels,
      threshold_default: 0.75,
      runtime: "onnx",
      path: path.join(this.modelsDir, `${config.id}.onnx`)
    };
  }

  // Validate model artifacts
  validateArtifacts(artifacts: RiceModelArtifacts): { valid: boolean; errors: string[] } {
    const errors: string[] = [];
    
    if (!fs.existsSync(artifacts.model_path)) {
      errors.push(`Model file not found: ${artifacts.model_path}`);
    }
    
    if (!fs.existsSync(artifacts.labels_path)) {
      errors.push(`Labels file not found: ${artifacts.labels_path}`);
    }
    
    if (!fs.existsSync(artifacts.meta_path)) {
      errors.push(`Metadata file not found: ${artifacts.meta_path}`);
    }
    
    if (!fs.existsSync(artifacts.onnx_path)) {
      errors.push(`ONNX file not found: ${artifacts.onnx_path}`);
    }

    return {
      valid: errors.length === 0,
      errors
    };
  }

  // Deploy model artifacts to production
  deployModel(artifacts: RiceModelArtifacts): boolean {
    try {
      const validation = this.validateArtifacts(artifacts);
      if (!validation.valid) {
        console.error("Model validation failed:", validation.errors);
        return false;
      }

      // Copy ONNX model to models directory
      const onnxDest = path.join(this.modelsDir, path.basename(artifacts.onnx_path));
      fs.copyFileSync(artifacts.onnx_path, onnxDest);
      
      // Copy labels and metadata
      const labelsDest = path.join(this.modelsDir, path.basename(artifacts.labels_path));
      const metaDest = path.join(this.modelsDir, path.basename(artifacts.meta_path));
      
      fs.copyFileSync(artifacts.labels_path, labelsDest);
      fs.copyFileSync(artifacts.meta_path, metaDest);

      console.log(`Model deployed successfully to ${this.modelsDir}`);
      return true;
    } catch (error) {
      console.error("Model deployment failed:", error);
      return false;
    }
  }

  // Get model performance metrics
  getModelMetrics(metricsPath: string): RiceTrainingMetrics[] {
    try {
      const metricsData = fs.readFileSync(metricsPath, 'utf8');
      return JSON.parse(metricsData);
    } catch (error) {
      console.error("Failed to load metrics:", error);
      return [];
    }
  }

  // Generate model report
  generateModelReport(config: RiceModelConfig, metrics: RiceTrainingMetrics[]): string {
    const bestEpoch = metrics.reduce((best, current) => 
      current.val_acc > best.val_acc ? current : best
    );
    
    return `
# Rice Model Report: ${config.id}

## Configuration
- Architecture: ${config.architecture}
- Input Size: ${config.input_size}x${config.input_size}
- Epochs: ${config.epochs}
- Learning Rate: ${config.learning_rate}
- Batch Size: ${config.batch_size}

## Training Results
- Best Validation Accuracy: ${(bestEpoch.val_acc * 100).toFixed(2)}%
- Best Epoch: ${bestEpoch.epoch}
- Final Training Accuracy: ${(metrics[metrics.length - 1]?.train_acc * 100 || 0).toFixed(2)}%
- Final Validation Accuracy: ${(metrics[metrics.length - 1]?.val_acc * 100 || 0).toFixed(2)}%

## Model Artifacts
- ONNX Model: ${config.id}.onnx
- Labels: labels.json
- Metadata: meta.json
- Configuration: ${config.id}_config.json
- Metrics: metrics.json

## Next Steps
1. Deploy model to production
2. Test with real rice leaf images
3. Monitor performance in production
4. Consider retraining with more data if needed
`;
  }
}

// Export singleton instance
export const riceModelManager = new RiceModelManager();
